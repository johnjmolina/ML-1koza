{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import theano as th\n",
    "import theano.tensor as tt\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "\n",
    "from plotutils import addtxt\n",
    "mpl.style.use(['./scripts/theme_bw.mplstyle', './scripts/presentation.mplstyle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume a straight-line model, the $k$-th data point is given by\n",
    "\n",
    "$\\begin{align}\n",
    "y_k &= m x_k + b\n",
    "\\end{align}$\n",
    "\n",
    "We are given a set of data points $\\{Y_k\\}$, with error bars $\\{\\sigma_k\\}$ at positions $\\{x_k\\}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(mu, sig, num):\n",
    "    def measure0(mu0):\n",
    "        draw = sig*np.random.randn(num) + mu0\n",
    "        return np.array([np.mean(draw), np.std(draw)])\n",
    "    dmy = np.array([measure0(mu0) for mu0 in mu])\n",
    "    return dmy[:,0], dmy[:,1]\n",
    "\n",
    "size = 100\n",
    "gold_intercept, gold_slope = 1,2\n",
    "\n",
    "x = np.linspace(0, 1, size)\n",
    "# y = mx + b\n",
    "gold_line = gold_slope * x + gold_intercept\n",
    "# add noise\n",
    "y,dy = measure(gold_line, 0.5, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,9))\n",
    "ax  = fig.add_subplot(111, xlabel=r'$x$', ylabel=r'$y$', title = 'Data and Model')\n",
    "ax.errorbar(x,y,yerr=dy,marker='x', ls='None', label='sampled data')\n",
    "ax.plot(x, gold_line, label='true data', lw=2.0)\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes theorem states that ($X$ the vector of model parameters, $D$ the vector of measured data, and $I$ all the prior information)\n",
    "\n",
    "$\\begin{align}\n",
    "P(X|D I) &\\propto P(D|X I) P(X|I) \\\\\n",
    "P(X|D I) &\\propto P(D|X I)\n",
    "\\end{align}$\n",
    "\n",
    "assuming uniform priors ($P(X|I) = \\text{constant}$). Furthermore, since the individual datas are considered independent of eachother, subject to independent gaussian noise, we have\n",
    "\n",
    "$\\begin{align}\n",
    "P(X | D I) &\\propto \\exp\\left(-\\frac{\\chi^2}{2}\\right) \\\\\n",
    "\\chi^2 &= \\sum_k\\left(\\frac{y_k - Y_k}{\\sigma_k}\\right)^2\n",
    "\\end{align}$\n",
    "\n",
    "where $y_k$ are the ideal (noiseless data, given the model parameters), whereas $Y_k$ are the measured noisy data values. The likelihood function is then given by\n",
    "\n",
    "$\\begin{align}\n",
    "L &= \\log{\\left[P(X| D, I)\\right]} = \\text{constant} - \\frac{\\chi^2}{2} \\\\\n",
    "\\nabla L &= - \\frac{1}{2}\\nabla\\chi^2\n",
    "\\end{align}$\n",
    "\n",
    "So the most probable value, the one that maximizes the likelihood is the one that minimized $\\chi^2$ (i.e., the least-square estimate), $\\nabla \\chi^2 = 0$\n",
    "\n",
    "$\\begin{align}\n",
    "\\chi^2 &= \\sum_k \\frac{\\left(m x_k + c - Y_k\\right)^2}{\\sigma_k^2}\\\\\n",
    "\\partial_m \\chi^2 &= \\sum_k \\frac{2 \\left(m x_k + c - Y_k\\right) x_k}{\\sigma_k^2} \\qquad\n",
    "&\\partial_c \\chi^2 &= \\sum_k \\frac{2 \\left(m x_k + c - Y_k\\right)}{\\sigma_k^2} \\\\\n",
    "\\nabla\\chi^2 &= \\begin{pmatrix}\\partial_m \\chi^2\\\\\\partial_c \\chi^2\\end{pmatrix} = \n",
    "\\begin{pmatrix}\\sum w_k x_k^2 & \\sum w_k x_k\\\\\n",
    "\\sum w_k x_k & \\sum w_k\n",
    "\\end{pmatrix}\\begin{pmatrix} m \\\\ c\\end{pmatrix} - \\begin{pmatrix}\\sum w_k x_k Y_k\\\\\n",
    "\\sum w_k Y_k\\end{pmatrix} \\\\\n",
    "&=\\begin{pmatrix}\\alpha & \\gamma \\\\\\gamma &\\beta\\end{pmatrix}\\begin{pmatrix}m \\\\ c\\end{pmatrix} - \\begin{pmatrix}p \\\\ q\\end{pmatrix}\n",
    "\\end{align}$\n",
    "\n",
    "with $w_k = 2/\\sigma_k^2$. The optimum solution is then\n",
    "\n",
    "$\\begin{align}\n",
    "\\nabla \\chi^2 &= 0 \\\\\n",
    "\\begin{pmatrix}m\\\\c\\end{pmatrix} &=\\begin{pmatrix}\\alpha & \\gamma\\\\\\gamma &\\beta\\end{pmatrix}^{-1} \\begin{pmatrix}p\\\\q\\end{pmatrix} = \\frac{1}{\\alpha\\beta - \\gamma^2} \\begin{pmatrix}\\beta &-\\gamma \\\\ -\\gamma & \\alpha\\end{pmatrix}\n",
    "\\begin{pmatrix}p\\\\q\\end{pmatrix} = \\frac{1}{\\alpha\\beta-\\gamma^2}\\begin{pmatrix}\n",
    "\\beta p - \\gamma q\\\\\\alpha q - \\gamma p\n",
    "\\end{pmatrix}\n",
    "\\end{align}$\n",
    "\n",
    "Finally, the covariance matrix can be obtained from the Hessian $\\boldsymbol{\\sigma^2} = -(\\nabla\\nabla L)^{-1} = 2 (\\nabla\\nabla \\chi^2)^{-1}$\n",
    "\n",
    "$\\begin{align}\n",
    "\\nabla\\nabla \\chi^2 &= \\begin{pmatrix}\\alpha & \\gamma\\\\\\gamma & \\beta\\end{pmatrix} \\\\\n",
    "\\boldsymbol{\\sigma}^2 &= 2\\frac{1}{\\alpha\\beta - \\gamma^2}\\begin{pmatrix}\n",
    "\\beta &-\\gamma\\\\-\\gamma & \\alpha\n",
    "\\end{pmatrix}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal(xval, yval, yerr):\n",
    "    w = 2 / yerr**2\n",
    "    alpha,beta,gamma = np.sum(w*xval**2), np.sum(w), np.sum(w*xval)\n",
    "    p,q  = np.sum(w*xval*yval), np.sum(w*yval)\n",
    "    idet = 1.0 / (alpha*beta - gamma**2)\n",
    "    m,c  = idet*(beta*p - gamma*q), idet*(alpha*q - gamma*p)\n",
    "    return np.array([m,c]), 2*idet*np.array([[beta, -gamma], [-gamma, alpha]])\n",
    "xopt, xcov = optimal(x, y, dy)\n",
    "print('m_opt = {0:.3f} +/- {1:.3e}'.format(xopt[0], np.sqrt(xcov[0][0])))\n",
    "print('c_opt = {0:.3f} +/- {1:.3e}'.format(xopt[1], np.sqrt(xcov[1][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It corresponds to the numerical solution of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2(p):\n",
    "    return np.sum((p[0]*x + p[1] - y)**2/dy**2)\n",
    "xopt, fopt, gopt, Bopt, fcalls, gcalls, warn = sp.optimize.fmin_bfgs(chi2, np.array([0.0, 0.0]), disp=1, full_output=1)\n",
    "print('m_opt = {0:.3f} +/- {1:.3e}'.format(xopt[0], np.sqrt(Bopt[0][0]*2)))\n",
    "print('c_opt = {0:.3f} +/- {1:.3e}'.format(xopt[1], np.sqrt(Bopt[1][1]*2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use MCMC to get the full distribution, not just the maximal likelihood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    #priors for unknown model parameters\n",
    "    m = pm.Uniform('m', lower=0, upper=10)\n",
    "    c = pm.Uniform('c', lower=0, upper=10)\n",
    "    yobs = pm.Normal('yobs', mu=m*x + c, sd=dy, observed=y)\n",
    "    trace = pm.sample(5000, tune = 10000, progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace[100:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.errorbar(x, y, dy, ls='None', label='data')\n",
    "niter = len(trace['m'])\n",
    "for _m,_c in zip(trace['m'][niter//2::5], trace['c'][niter//2::5]):\n",
    "    plt.plot(x, _m*x + _c, c='gray', alpha=0.1)\n",
    "plt.plot(x, gold_line, label='true regression line', lw=3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
