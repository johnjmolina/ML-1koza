{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import theano as th\n",
    "import theano.tensor as tt\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "\n",
    "from plotutils import addtxt\n",
    "mpl.style.use(['./scripts/theme_bw.mplstyle', './scripts/presentation.mplstyle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume a straight-line model, the $k$-th data point is given by\n",
    "\n",
    "$\\begin{align}\n",
    "y_k &= m x_k + b\n",
    "\\end{align}$\n",
    "\n",
    "We are given a set of data points $\\{Y_k\\}$, at positions $\\{x_k\\}$, but we are not given the errors this time, although we do assume it is gaussian.\n",
    "\n",
    "$\\begin{align}\n",
    "P(X|D I) &= \\int\\text{d}\\sigma P(X\\sigma | DI) \\\\\n",
    "&= \\int\\text{d}\\sigma \\frac{P(D | X \\sigma I) P(X\\sigma | I)}{P(D|I)} \\\\\n",
    "&\\propto \\int\\text{d}\\sigma P(D|X\\sigma I)P(X\\sigma|I)\n",
    "\\end{align}$\n",
    "\n",
    "For independent gaussian noise we still have\n",
    "\n",
    "$\\begin{align}\n",
    "P(D|X\\sigma I) &\\propto \\frac{1}{\\sigma^N} \\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum\\left(m x_k +c - Y_k\\right)^2\\right]} \n",
    "\\end{align}$\n",
    "\n",
    "so the likelihood is given as (assuming uniform priors)\n",
    "\n",
    "$\\begin{align}\n",
    "P(X | D I) &\\propto \\int\\text{d}\\sigma \\sigma^{-N} \\exp\\left[-\\frac{1}{\\sigma^2}\\sum\\left(m x_k + c - Y_k\\right)^2\\right] \\\\\n",
    "&=\\left[\\sum \\left(m x_k + c - Y_k\\right)^2\\right]^{-(N-1)/2}\n",
    "\\end{align}$\n",
    "\n",
    "and the log likelihood becomes\n",
    "\n",
    "$\\begin{align}\n",
    "L &= \\text{constant} - \\frac{N-1}{2}\\log{\\chi^2}\\\\\n",
    "\\end{align}$\n",
    "\n",
    "To maximize this likelihood, we should again minimize $\\chi^2$, just as in the case when the errors $\\sigma_k$ are given.\n",
    "\n",
    "$\\begin{align}\n",
    "\\nabla L &= -\\frac{N-1}{2}\\frac{1}{\\chi^2}\\nabla\\chi^2\\\\\n",
    "&= -\\frac{1}{2}\\left(\\frac{N-1}{\\chi^2}\\right)\\left\\{\\begin{pmatrix}\\alpha^\\prime & \\gamma^\\prime \\\\\\gamma^\\prime & \\beta^\\prime\\end{pmatrix}\\begin{pmatrix}m \\\\c\\end{pmatrix} -\\begin{pmatrix}p^\\prime\\\\q^\\prime\\end{pmatrix}\\right\\}\n",
    "\\end{align}$\n",
    "\n",
    "We recover the same optimum solution ($\\nabla L = \\nabla\\chi^2 \\equiv 0$), given by\n",
    "\n",
    "$\\begin{align}\n",
    "\\begin{pmatrix}m_0\\\\c_0\\end{pmatrix} &= \\frac{1}{\\alpha^\\prime \\beta^\\prime - \\gamma^{\\prime 2}}\\begin{pmatrix}\\beta^\\prime p^\\prime - \\gamma^\\prime q^\\prime \\\\\\alpha^\\prime q^\\prime - \\gamma^\\prime p^\\prime\\end{pmatrix}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{align}\n",
    "\\nabla\\nabla L &= -\\frac{1}{2}\\frac{N-1}{\\chi^2}\\nabla\\nabla\\chi^2 + \\frac{1}{2}\\frac{N-1}{\\left(\\chi^2\\right)^{-2}}\\nabla\\chi^2 \\\\\n",
    "\\nabla\\nabla L(\\boldsymbol{X}_0) &= -\\frac{1}{2}\\frac{N-1}{\\chi^2(\\boldsymbol{X_0})}\\nabla\\nabla\\chi^2 = -\\frac{1}{2}\\nabla\\nabla\\frac{\\chi^2}{S^2}\n",
    "\\end{align}$\n",
    "\n",
    "where we defined $S^2$ as the estimate of $\\sigma^2$, equal for all data points, and recovered exactly the same solution as the case when the $\\sigma_k$ are specified, i.e.,\n",
    "\n",
    "$\\begin{align}\n",
    "\\alpha &= \\alpha^\\prime / S^2, \\qquad \\beta = \\beta^\\prime/S^2, \\qquad \\gamma = \\gamma^\\prime/S^2, \\qquad p = p^\\prime / S^2, \\qquad q = q^\\prime / S^2, \\qquad w=w_k = 2/S^2, \\qquad \\sigma_k^2 = S^2 = \\frac{1}{N-1}\\sum\\left(m_0 x_k + c_0 - Y_k\\right)^2\n",
    "\\end{align}$\n",
    "\n",
    "Finally, the covariance matrix is given by\n",
    "\n",
    "$\\begin{align}\n",
    "\\boldsymbol{\\sigma}^2 &= -\\left(\\nabla\\nabla L\\right)^{-1} = 2 S^2 \\left(\\nabla\\nabla\\chi^2\\right)^{-1} \\\\\n",
    "&= 2\\frac{\\chi^2(\\boldsymbol{X_0})}{N-1}\\left(\\nabla\\nabla\\chi^2\\right)^{-1}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 200\n",
    "gold_intercept, gold_slope = 1,2\n",
    "\n",
    "x = np.linspace(0, 1, size)\n",
    "# y = m x + b\n",
    "gold_line = gold_slope * x + gold_intercept\n",
    "# add noise\n",
    "y = gold_line + np.random.normal(scale=0.5, size=size)\n",
    "\n",
    "data = dict(x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(111, xlabel='x', ylabel='y', title='Generated data')\n",
    "ax.plot(x, y, 'x', label='sampled data')\n",
    "ax.plot(x, gold_line, label='true line', lw=2.)\n",
    "plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical solution\n",
    "def chi2(p):\n",
    "    return np.sum((p[0]*x + p[1] - y)**2)\n",
    "def optimal(xval, yval):\n",
    "    N = len(xval)\n",
    "    alpha, beta, gamma = np.sum(xval**2), N, np.sum(xval)\n",
    "    p, q = np.sum(xval*yval), np.sum(yval)\n",
    "    idet = 1.0 / (alpha*beta - gamma**2)\n",
    "    m, c = idet*(beta*p - gamma*q), idet*(alpha*q - gamma*p)\n",
    "    s2   = np.sum((m*xval + c - yval)**2)/(N-1)\n",
    "    return np.array([m,c]), 2*s2*idet*np.array([[beta, -gamma], [-gamma, alpha]])\n",
    "xopt, xcov = optimal(x,y)\n",
    "print('chi2= {0:.3e}'.format(chi2(xopt)))\n",
    "print('m_0 = {0:.2f} +/- {1:.2e}'.format(xopt[0], np.sqrt(xcov[0][0])))\n",
    "print('c_0 = {0:.2f} +/- {1:.2e}'.format(xopt[1], np.sqrt(xcov[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical minimization\n",
    "xopt, fopt, gopt, Bopt, fcalls, gcalls, warn = sp.optimize.fmin_bfgs(chi2, np.array([0.0, 0.0]), disp=1, full_output=1)\n",
    "xcov  = 2*fopt/len(x)*Bopt\n",
    "print('chi2= {0:.3e}'.format(fopt))\n",
    "print('m_0 = {0:.2f} +/- {1:.2e}'.format(xopt[0], np.sqrt(xcov[0][0])))\n",
    "print('c_0 = {0:.2f} +/- {1:.2e}'.format(xopt[1], np.sqrt(xcov[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC\n",
    "with pm.Model() as model:\n",
    "    #priors for unkown model parameters\n",
    "    m = pm.Uniform('m', lower=0, upper=10)\n",
    "    c = pm.Uniform('c', lower=0, upper=10)\n",
    "    s = pm.Uniform('s', lower=0, upper=10)\n",
    "    yobs = pm.Normal('yobs', mu=m*x + c, sd=s, observed = y)\n",
    "    trace = pm.sample(10000, tune = 10000, progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace[5000:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,7))\n",
    "pm.forestplot(trace);\n",
    "plt.axvline(0.5, 0, 1/3, c='C1')\n",
    "plt.axvline(1.0, 0, 2/3, c='C1')\n",
    "plt.axvline(2.0, 0, 1, c='C1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, kde_plot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(x, y,'x', label='data')\n",
    "niter = len(trace['m'])\n",
    "for _m,_c in zip(trace['m'][niter//2::5], trace['c'][niter//2::5]):\n",
    "    plt.plot(x, _m*x + _c, c='gray', alpha=0.1)\n",
    "plt.plot(x, gold_line, label='true regression line', lw=3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
