{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy        as np\n",
    "import pandas       as pd\n",
    "import dynesty      as dyn\n",
    "import matplotlib   as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate\n",
    "from scipy.special import factorial\n",
    "from plotutils import addtxt, colorbar\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from dynesty import plotting as dyplot\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "mpl.style.use(['./scripts/theme_bw.mplstyle', './scripts/presentation.mplstyle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\mathcal{Z} &= \\int_0^1 \\mathcal{L}(X) \\text{d}X \\simeq \\sum_{k=1}^N A_k \\\\\n",
    "A_k &= w_k \\mathcal{L}_k \\\\\n",
    "w_k &= X_{k-1} - X_k \\\\\n",
    "w_k &\\simeq e^{-k/n}\\qquad (n: \\text{number of live points})\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{H} &= \\int P(X)\\log{\\left[P(X)\\right]} \\text{d}X \\simeq \\sum_{k=1}^N \\frac{A_k}{\\mathcal{Z}} \\log{\\left[\\frac{\\mathcal{L}_k}{\\mathcal{Z}}\\right]}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logAdd(logx,logy):\n",
    "        \"\"\"logarithmic addition log(exp(logx) + exp(logy))\"\"\"\n",
    "        if logx > logy:\n",
    "            return logx + np.log(1.0 + np.exp(logy-logx))\n",
    "        else:\n",
    "            return logy + np.log(1.0 + np.exp(logx-logy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmap(func, *iterables):\n",
    "    return np.array(list(map(func, *iterables)))\n",
    "\n",
    "def nestedSampler(*, n, maxIter, newObj, prior, logLikelihood, explorer, mcparams):\n",
    "    \"\"\"Adapted from Sivia and Skilling, Data Analysis : A Bayesian Tutorial, pg. 188\"\"\"\n",
    "    \n",
    "    obj         = newObj(n)                     # live points θ\n",
    "    logL        = np.zeros(n)                   # log(L) for live points\n",
    "\n",
    "    sample      = newObj(maxIter)               # dead points θ\n",
    "    logLsample  = np.zeros(maxIter)             # log(L) for dead points\n",
    "    logWtsample = np.zeros(maxIter)             # log(weight) for dead points\n",
    "\n",
    "    H           = 0.0                           # information, initially 0\n",
    "    logZ        = np.finfo(np.double).min       # log(Evidence Z), initially 0\n",
    "    logWidth    = np.log(1.0 - np.exp(-1.0/n))  # log(width in prior mass), initially set to outermost interval of prior mass\n",
    "\n",
    "    obj[...]    = prior(n)                      # sample objects from prior Π(θ)\n",
    "    logL[...]   = fmap(logLikelihood, obj)      # compute likelihood of all points\n",
    "    for nest in trange(maxIter):\n",
    "        worst   = np.argmin(logL)               # find worst object in collection, with Weight = width * likelihood\n",
    "        logWt   = logWidth + logL[worst]        # weight (width * likelihood) of new dead point\n",
    "        \n",
    "        logZnew = logAdd(logZ, logWt)           # update evidence Z and Information H\n",
    "        H       = np.exp(logWt - logZnew) * logL[worst] + np.exp(logZ - logZnew) * (H + logZ) - logZnew\n",
    "        logZ    = logZnew\n",
    "\n",
    "        sample[nest]      = obj[worst]          # add worst point to list of dead points (posterior samples)\n",
    "        logLsample[nest]  = logL[worst]      \n",
    "        logWtsample[nest] = logWt\n",
    "                \n",
    "        copy = np.random.randint(low=0, high=n) # kill worst object in favor of copy of different survivor\n",
    "        while copy == worst and n > 1:\n",
    "            copy = np.random.randint(low=0, high=n)\n",
    "        \n",
    "        logLstar    = logL[worst]               # new likelihood constraint\n",
    "        obj[worst]  = obj[copy]                 # \"worst\" is now new live point\n",
    "        explorer(x=obj[worst], logLikelihood=logLikelihood, logLstar=logLstar, params=mcparams)\n",
    "        logL[worst] = logLikelihood(obj[worst])\n",
    "        \n",
    "        logWidth   -= 1.0/n                       # shrink interval\n",
    "\n",
    "    print(f'Iterate #     = {nest:7d}')\n",
    "    print(f'Evidence logZ = {logZ:12.6g} +/- {np.sqrt(H/n):8.6g}')\n",
    "    print(f'Information H = {H:12.6g} nats = {H/np.log(2.0):12.6g} bits')\n",
    "    return obj, sample, logLsample, logWtsample, H, logZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d-dimensional Gaussian\n",
    "\n",
    "\\begin{align}\n",
    "L(\\Theta) &= \\exp{\\left(-\\frac{r^2}{2\\sigma^2}\\right)}, \\qquad r^2 = \\Theta^2 = \\sum_i^d \\theta_i^2\n",
    "\\end{align}\n",
    "\n",
    "where $\\Theta$ has a flat prior within the unit d-ball. Since the volume of a d-ball with radius $R$ is (assuming $d$ is even)\n",
    "\\begin{align}\n",
    "V_d &= \\frac{\\pi^{d/2}}{\\left(\\frac{d}{2}\\right)!} R^d ,\\qquad (\\pi = 3.1415\\ldots)\n",
    "\\end{align}\n",
    "\n",
    "the the prior is\n",
    "\\begin{align}\n",
    "\\Pi(\\Theta) &= \\frac{\\left(d/2\\right)!}{\\pi^{d/2}} = 1 / V_{d}(R=1)\n",
    "\\end{align}\n",
    "\n",
    "We take $d = 10$, and $\\sigma = 0.01$ so all the likelihood is well within the prior domain. The evidence, discarding the tails outside the domains is\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{Z} &= \\int \\mathcal{L}(\\Theta) \\Pi(\\Theta)\\,\\text{d}\\Theta \\\\\n",
    "&\\simeq \\Pi(\\Theta) \\int \\exp{\\left[-\\frac{1}{2}\\Theta^\\text{t} \\Sigma^{-1} \\Theta\\right]} \\text{d}\\Theta,\\qquad \\Sigma^{-1} = \\frac{1}{\\sigma^2} I \\\\\n",
    "&= \\Pi(\\Theta) \\sqrt{(2\\pi)^d \\det\\Sigma} \\\\\n",
    "&= \\frac{(d/2)!}{\\pi^{d/2}} (2\\pi)^{d/2} \\sigma^{d} \\\\\n",
    "&= \\left(d/2\\right)! \\left(2 \\sigma^2\\right)^{d/2} \\\\\n",
    "\\log{\\mathcal{Z}} &= \\frac{d}{2}\\log{\\left(2 \\sigma^2\\right)} + \\log{\\left(d/2\\right)!}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $L$ is a decreasing function of $r$, if we could perform the sorging, we would organize $\\theta$ is radial sequences of nested shells. In this case we can exactly compute the prior mass\n",
    "\\begin{align}\n",
    "X(\\lambda) &= \\int_{\\left\\{\\mathcal{L}(\\Theta) > \\lambda\\right\\}} \\Pi(\\Theta) \\text{d}\\Theta \\\\\n",
    "&= \\Pi(\\Theta)\\int_{r^2 \\le -2\\sigma^2\\log(\\lambda)} \\text{d}\\Theta \\\\\n",
    "&= \\Pi(\\Theta) V_d\\left(R = \\sqrt{-2\\sigma^2\\log{\\lambda}}\\right) \\\\\n",
    "&= (-2\\sigma^2\\log\\lambda)^{d/2}\n",
    "\\end{align}\n",
    "Thus, we can invert $X(\\lambda)$ to yield\n",
    "\\begin{align}\n",
    "L(X) &= \\exp{\\left[-\\frac{X^{2/d}}{2\\sigma^2}\\right]}\n",
    "\\end{align}\n",
    "\n",
    "Recall that what we want to evaluate is\n",
    "\\begin{align}\n",
    "\\mathcal{Z} &= \\int_0^1 \\mathcal{L}(X) \\text{d}X\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLikelihood(θ):\n",
    "    \"\"\"L(θ)\"\"\"\n",
    "    return -np.sum(θ**2) / (2*σ**2) \n",
    "\n",
    "def prior(*, num, dim):\n",
    "    \"\"\"Return num points uniformly sampled in unit dim-ball\"\"\"\n",
    "    def marsaglia():\n",
    "        \"\"\"Return n points sampled over surface of unit dim-ball\"\"\"\n",
    "        r = np.random.randn(num, dim)\n",
    "        return r / np.linalg.norm(r, axis=1)[...,None]\n",
    "    x = marsaglia()\n",
    "    u = np.random.rand(num)**(1.0/dim)\n",
    "    x[...] = u[:,None]*x[...]\n",
    "    return x\n",
    "\n",
    "def normalize(a):\n",
    "    return a / np.max(np.abs(a))\n",
    "\n",
    "def mcexplorerUnitSphere(*, x, logLikelihood, logLstar, params):\n",
    "    x[:] = np.sqrt(-logLstar*2*σ**2)*prior(num=1, dim=d)[0]\n",
    "\n",
    "d,σ =10,0.01\n",
    "def logLikelihoodX(X):\n",
    "    \"\"\"L(X), the likelihood countour enclosing prior mass X\"\"\"\n",
    "    return -X**(2/d)/(2*σ**2)\n",
    "\n",
    "def logPriorMassL(logλ):\n",
    "    \"\"\"X(λ)\"\"\"\n",
    "    return d/2*(np.log(2*σ**2) + np.log(-logλ))\n",
    "\n",
    "logX = np.linspace(-70, 0, num=250)\n",
    "θ    = prior(num=100, dim=d)             # uniform samples\n",
    "logL = fmap(logLikelihood, θ)            # likelihood of samples\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,9))\n",
    "ax.plot(-logX, np.exp(logLikelihoodX(np.exp(logX))), label='prior')\n",
    "ax.plot(-logX, normalize(np.exp(logLikelihoodX(np.exp(logX)) + logX)), label='posterior')\n",
    "ax.plot(-logPriorMassL(logL), np.exp(logL), marker='x', ls='None')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 70)\n",
    "ax.set_xlabel(r'$-\\log{X}$')\n",
    "ax.set_ylabel(r'$\\mathcal{L}(X)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npoints  = 1\n",
    "nsamples = npoints*100\n",
    "params   = {'δ':0.1, 'steps':100, 'a':-1, 'b':1}\n",
    "live, sample, logLsample, logWtsample, H, logZ = nestedSampler(n=npoints, maxIter=nsamples, newObj = lambda n: np.zeros((n,d)), prior=lambda n : prior(num=n, dim=d), logLikelihood=logLikelihood, explorer=mcexplorerUnitSphere, mcparams=params)\n",
    "print('')\n",
    "print(f'log(Z_theory) = {d/2*np.log(2*σ**2) + np.log(factorial(d//2)):.6f}')\n",
    "print(f'Z_theory      = {factorial(d//2) * (2*σ**2)**(d//2):.6e}')\n",
    "fig, ax = plt.subplots(figsize=(12,9))\n",
    "ax.plot(-logX, np.exp(logLikelihoodX(np.exp(logX))))\n",
    "ax.plot(np.arange(0,nsamples)/npoints, np.exp(logLsample), marker='x', ls='None')\n",
    "ax.set_xlim(0,70)\n",
    "ax.set_xlabel(r'$-\\log{X}$')\n",
    "ax.set_ylabel(r'$\\mathcal{L}(X)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try same problem, but now using a general MCMC sampler to generate the new points. For this let us slightly change the problem and assume a uniform prior over a hyper-cube (instead of hyper-sphere) with sides (-1,1)\n",
    "\\begin{align}\n",
    "\\mathcal{Z} &= \\int \\mathcal{L}(\\Theta) \\Pi(\\Theta)\\,\\text{d}\\Theta \\\\\n",
    "&= \\Pi(\\Theta) \\int \\exp{\\left[-\\frac{1}{2}\\Theta^\\text{t} \\Sigma^{-1} \\Theta\\right]} \\text{d}\\Theta,\\qquad \\Sigma^{-1} = \\frac{1}{\\sigma^2} I \\\\\n",
    "&= \\Pi(\\Theta) \\sqrt{(2\\pi)^d \\det\\Sigma} \\\\\n",
    "&= \\frac{1}{2^d}\\sqrt{(2\\pi)^d \\sigma^{2n}} = \\left(\\frac{\\pi}{2}\\right)^{d/2} \\sigma^n\\\\\n",
    "\\log{\\mathcal{Z}} &= \\frac{d}{2}\\log{\\frac{\\pi}{2}} + n\\log{\\sigma}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'log(Z_theory) = {d/2*np.log(np.pi/2) + d*np.log(σ):.6f}')\n",
    "print(f'Z_theory      = {(np.pi/2)**(d//2) * σ**d:.6e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcexplorer(*, x, logLikelihood, logLstar, params):\n",
    "    \"\"\"MCMC explorer in d-cube over (a,b) \"\"\"\n",
    "    def tounitcube(z, lo=params['a'], hi=params['b']): \n",
    "        \"\"\"Map from (lo,hi) to (0,1)\"\"\"\n",
    "        return (z - lo)/(hi-lo)\n",
    "    def fromunitcube(z, lo=params['a'], hi=params['b']):\n",
    "        \"\"\"Map from (0,1) to (lo,hi)\"\"\"\n",
    "        return (hi-lo)*z + lo\n",
    "    def uniform():\n",
    "        \"\"\"Uniform random number [-1,1) \"\"\"\n",
    "        return fromunitcube(np.random.rand(len(x)), lo=-1, hi=1)\n",
    "\n",
    "    u    = tounitcube(x)\n",
    "    v    = np.zeros_like(x)\n",
    "    y    = np.zeros_like(x)\n",
    "    accept, reject = 0, 0\n",
    "    for i in range(params['steps']):\n",
    "        v[:] = u[:] + params['δ']*uniform() # new trial point\n",
    "        v[:]-= np.floor(v)\n",
    "        y[:] = fromunitcube(v)\n",
    "        logL = logLikelihood(y)\n",
    "        if(logL > logLstar):                # accept iff within hard likelihood constraint\n",
    "            u[:] = v[:]\n",
    "            x[:] = y[:]\n",
    "            accept += 1\n",
    "        else:\n",
    "            reject += 1\n",
    "        if accept > reject: # modify step size to get convergence rate of 50%\n",
    "            params['δ'] *= np.exp(1.0 / accept)\n",
    "        if accept < reject:\n",
    "            params['δ'] /= np.exp(1.0 / reject)\n",
    "        params['δ'] = np.min([params['δ'], (params['b']-params['a'])*0.5])\n",
    "def prior(*,num,dim):\n",
    "    return 2.0*np.random.rand(num,dim)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npoints  = 500\n",
    "nsamples = npoints*50\n",
    "params   = {'δ':0.2, 'steps':100, 'a':-1, 'b':1}\n",
    "live, sample, logLsample, logWtsample, H, logZ = nestedSampler(n=npoints, maxIter=nsamples, newObj = lambda n: np.zeros((n,d)), prior=lambda n : prior(num=n, dim=d), logLikelihood=logLikelihood, explorer=mcexplorer, mcparams=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how to solve the same problem using the \"dynesty\" package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def priorTransform(u):\n",
    "    \"\"\"Transforms our defualt unit cube samples `u` to a flat prior between -1. and 1. in each variable.\"\"\"\n",
    "    return (2. * u - 1.)\n",
    "\n",
    "# Vanilla nested sampler (as above)\n",
    "sampler = dyn.NestedSampler(logLikelihood, priorTransform, d, bound='single')\n",
    "sampler.run_nested(dlogz=0.01)\n",
    "res     = sampler.results\n",
    "\n",
    "# Dynamic nested sampler (fancier!)\n",
    "dsampler = dyn.DynamicNestedSampler(logLikelihood, priorTransform, d, bound='single', sample='unif')\n",
    "dsampler.run_nested(nlive_init=50, nlive_batch=50, maxiter=res.niter+res.nlive, use_stop=False, wt_kwargs={'pfrac': 0.0})\n",
    "dres_z = dsampler.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = dyplot.cornerplot(dres_z, quantiles=None, color='darkviolet',span=[[-.2,.2], [-.2, .2], [-.2,.2], [-.2,.2], [-.2,.2], [-.2,.2], [-.2, .2], [-.2,.2], [-.2,.2], [-.2,.2]],fig=plt.subplots(10, 10, figsize=(30, 30)))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egg-Basket\n",
    "\n",
    "Define the Egg-box likelihood $\\mathcal{L}(x,y) \\propto p(D|\\theta)$, where $\\theta=(x,y)$ are the model parameters, as\n",
    "\\begin{align}\n",
    "\\mathcal{L}(x,y) &= \\exp{\\left[\\left(2 + \\cos{\\left(x\\right)}\\cos{\\left(y\\right)}\\right)^5\\right]}\n",
    "\\end{align}\n",
    "with a uniform prior  $U(0,6\\pi)$ for both $x$ and $y$. Note the system is periodic in both $x$ and $y$ (period $6\\pi$).\n",
    "\n",
    "The Bayesian evidence integral is\n",
    "\\begin{align}\n",
    "\\mathcal{Z} &= \\int \\mathcal{L}(\\theta)\\pi(\\theta)\\text{d}\\theta\n",
    "= \\frac{1}{(6\\pi)^2}\\int\\mathcal{L}(\\theta)\\text{d}\\theta\\\\\n",
    "\\log\\mathcal{Z} &= \\log{\\left[\\int\\mathcal{L}(\\theta)\\text{d}\\theta\\right]} - 2\\log{6\\pi}\n",
    "\\simeq 235.856\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLikelihood(θ):\n",
    "    return (2.0 + np.cos(θ[0]) * np.cos(θ[1])) ** 5.0\n",
    "a,b= 0.0,6.0*np.pi\n",
    "X,Y= np.meshgrid(np.linspace(a,b,1000),np.linspace(a,b,1000),indexing='ij')\n",
    "Z  = integrate.dblquad(lambda x,y : np.exp(logLikelihood(np.array([x,y]))), a, b, lambda x: a, lambda x: b)\n",
    "print(f'Z = {np.log(Z[0]) - 2*np.log(b-a):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "im = ax.pcolormesh(X,Y,logLikelihood(np.stack([X,Y])))\n",
    "ax.set_xlabel(r'$x$', fontsize=22)\n",
    "ax.set_ylabel(r'$y$', fontsize=22)\n",
    "colorbar(ax, im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(*,num,dim):\n",
    "    return 6.0*np.pi*np.random.rand(num,dim)\n",
    "d        = 2\n",
    "npoints  = 1000\n",
    "nsamples = npoints*100\n",
    "params   = {'δ':0.1, 'steps':100, 'a':a, 'b':b}\n",
    "live, sample, logLsample, logWtsample, H, logZ = nestedSampler(n=npoints, maxIter=nsamples, newObj = lambda n: np.zeros((n,d)), prior=lambda n : prior(num=n, dim=d), logLikelihood=logLikelihood, explorer=mcexplorer, mcparams=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,9))\n",
    "ax.plot(np.arange(0,nsamples)/npoints, np.exp(logLsample), marker='x', ls='None')\n",
    "ax.set_xlim(0,10)\n",
    "ax.set_xlabel(r'$-\\log{X}$')\n",
    "ax.set_ylabel(r'$\\mathcal{L}(X)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "im = ax.pcolormesh(X,Y,logLikelihood(np.stack([X,Y])))\n",
    "colorbar(ax, im)\n",
    "ax.plot(sample[:,0], sample[:,1], ls='None', marker='x')\n",
    "ax.set_xlabel(r'$x$', fontsize=22)\n",
    "ax.set_ylabel(r'$y$', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=sample[:,0], y=sample[:,1], kind=\"hex\", color=\"k\" );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def priorTransform(u):\n",
    "    \"\"\"Transforms our defualt unit cube samples `u` to a flat prior between 0 and 6pi in each variable.\"\"\"\n",
    "    return u*6.0*np.pi\n",
    "dsampler = dyn.DynamicNestedSampler(logLikelihood, priorTransform, ndim=d, bound='multi', sample='unif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus on deriving the evidence\n",
    "dsampler.run_nested(dlogz_init=0.01, nlive_init=500, nlive_batch=500,wt_kwargs={'pfrac': 0.0}, stop_kwargs={'pfrac': 0.0})\n",
    "dres_z = dsampler.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now add samples to get the posterior\n",
    "dsampler.reset()\n",
    "dsampler.run_nested(dlogz_init=0.01, nlive_init=500, nlive_batch=500,wt_kwargs={'pfrac': 1.0}, stop_kwargs={'pfrac': 1.0})\n",
    "dres_p = dsampler.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = dyplot.runplot(dres_z, color='blue')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = dyplot.cornerplot(dres_p, quantiles=None, color='darkviolet',span=[[0, 6*np.pi], [0, 6*np.pi]],fig=plt.subplots(2, 2, figsize=(10, 10)))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "im = ax.pcolormesh(X,Y,logLikelihood(np.stack([X,Y])))\n",
    "colorbar(ax, im)\n",
    "ax.plot(dres_z.samples[:,0], dres_z.samples[:,1], ls='None', marker='x')\n",
    "ax.set_xlabel(r'$x$', fontsize=22)\n",
    "ax.set_ylabel(r'$y$', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "im = ax.pcolormesh(X,Y,logLikelihood(np.stack([X,Y])))\n",
    "colorbar(ax, im)\n",
    "ax.plot(dres_p.samples[:,0], dres_p.samples[:,1], ls='None', marker='x')\n",
    "ax.set_xlabel(r'$x$', fontsize=22)\n",
    "ax.set_ylabel(r'$y$', fontsize=22)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
