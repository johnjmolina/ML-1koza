{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import autograd.numpy as anp  # Thinly-wrapped numpy\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "import theano as th\n",
    "import theano.tensor as tt\n",
    "import warnings\n",
    "import pprint\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import optimize\n",
    "from pymc3.gp.util import plot_gp_dist\n",
    "from autograd import grad     # The only autograd function you may ever need\n",
    "from autograd import elementwise_grad as egrad\n",
    "\n",
    "from plotutils import addtxt\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "mpl.style.use(['./scripts/theme_bw.mplstyle', './scripts/presentation.mplstyle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_SquareExp(x1,x2,l=1.0):\n",
    "    return np.exp(-0.5*(np.subtract.outer(x1,x2)/l)**2)\n",
    "\n",
    "def GP(μ, K, noise): # Rasmussen pg. 201\n",
    "    return np.random.multivariate_normal(μ, K + np.diag(noise))\n",
    "\n",
    "def GPfast(μ, K, noise): # Rasmussen pg. 201\n",
    "    L = np.linalg.cholesky(K + np.diag(noise)) # adding small noise makes calculation more stable\n",
    "    u = np.random.randn(len(μ))\n",
    "    return μ + np.dot(L, u)\n",
    "\n",
    "def GPpost0(x_obs, y_obs, x_new, μ, K_obs_obs, K_new_obs, K_new_new): # Rasmussen pg. 16\n",
    "    # return posterior distribution for test data at x* (y*), conditioned on training data at x (y)\n",
    "    # p(y* | y, x) = GP(μ(x*) + (K*)^t K \\ δy, K** - (K*)^t K \\ K*) \n",
    "    L = np.linalg.cholesky(K_obs_obs)\n",
    "\n",
    "    # α = K \\ δy(x) = L^t \\ (L | δy)\n",
    "    α = np.linalg.solve(L.transpose(), np.linalg.solve(L, y_obs - μ(x_obs)))\n",
    "\n",
    "    # μpost = μ(x*) + (K*)^t K \\ δy(x)\n",
    "    μpost = μ(x_new) + np.dot(K_new_obs, α) # posterior avg\n",
    "    \n",
    "    # Kpost = K** - (K*)^t K | K* = K** - W\n",
    "    # W_ij  = v_i . v_j \n",
    "    # v_i   = (L | c_i) . (L | c_j); c_i the i-th column of K*, i-th row of (K*)^t\n",
    "    V     = np.array([np.linalg.solve(L, c) for c in K_new_obs]) # V= [v_1, v_2, ...]^t\n",
    "    Kpost = K_new_new - np.einsum('ik,jk->ij', V, V)\n",
    "    return μpost, Kpost\n",
    "\n",
    "def GPpost(x_obs, y_obs, x_new, μ, K, noise_obs): # Rasmussen pg. 16\n",
    "    # return posterior distribution for test data at x* (y*), conditioned on training data at x (y)\n",
    "    # p(y* | y, x) = GP(μ(x*) + (K*)^t K \\ δy, K** - (K*)^t K \\ K*) \n",
    "    K_obs_obs  = K(x_obs, x_obs) + np.diag(noise_obs) # K\n",
    "    K_new_new  = K(x_new, x_new) # K**\n",
    "    K_new_obs  = K(x_new, x_obs) # (K*)^t\n",
    "    return GPpost0(x_obs, y_obs, x_new, μ, K_obs_obs, K_new_obs, K_new_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative = lambda func,x: egrad(func)(x)\n",
    "def tangent(f,x0):\n",
    "    m = grad(f)(x0)\n",
    "    b = f(x0) - m*x0\n",
    "    return lambda z: m*z + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 1e-3*(x+5)*(x-2)*(x-7)*(x+0.4)*(x-1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(666)\n",
    "a,b= -6,6\n",
    "xt = (a + np.random.rand(10)*(b-a)*0.5) + (b-a)*0.25    # training data on function\n",
    "xs = np.array([-5.5, -4, 5, 7.5])                       # training data on derivative\n",
    "nums = {'obs':len(xt) + len(xs), 'new':500}\n",
    "\n",
    "\n",
    "fig, [ax,bx] = plt.subplots(figsize=(18,6), ncols=2, sharex=True, sharey=True)\n",
    "x  = np.linspace(-10,10,num=nums['new'])\n",
    "# function\n",
    "ax.plot(x, f(x))\n",
    "ax.plot(xt, f(xt), marker='o', mfc='None', ls='None', mew=2, color=\"C0\", label=r'training $f$')\n",
    "\n",
    "# derivative\n",
    "bx.plot(x,f(x))\n",
    "bx.plot(xs, f(xs), marker='o', mfc='None', ls='None', mew=3, color=\"C3\", label=r'training $f^\\prime$')\n",
    "for x0 in xs:\n",
    "    fprime = tangent(f,x0)\n",
    "    dx     = np.linspace(x0-0.3, x0+0.3)\n",
    "    bx.plot(dx,fprime(dx), color=\"C3\",lw=5,alpha=0.8)\n",
    "ax.set_ylim(-4,2)\n",
    "ax.legend(fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the posterior for the hyperparameters is of the form\n",
    "\\begin{align}\n",
    "\\log{P(\\eta,l | D, I)} &= -\\frac{1}{2}\\left[y^t K^{-1}(\\eta, l) y + \\log{\\det{K(\\eta,l)}} + n \\log(2\\pi)\\right] \n",
    "- \\left[\\log{l} + \\log{\\eta}\\right]\n",
    "\\end{align}\n",
    "where we have assumed that $\\pi(\\theta) = \\pi(l, \\eta)= \\pi(l) \\pi(\\eta)$ and $\\mu = 0$. Maximizing this expression, gives the optimal $\\eta$ and $l$ values\n",
    "for our Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mmas.github.io/optimization-scipy\n",
    "def gethyperparams(x, y, noise, K):\n",
    "    def logGP(*,η,l):\n",
    "        L = η*np.linalg.cholesky(K(x,x,l=l) + np.diag(noise))\n",
    "        v = np.linalg.solve(L, y)\n",
    "        return 0.5*np.dot(v,v) + np.sum(np.log(np.diag(L))) # 1/2 [ y K \\ y + log|K|]\n",
    "    \n",
    "    logp = lambda p: (logGP(η = p[0], l = p[1]) + np.log(p[0]) + np.log(p[1])) # - log P(η, l | D)\n",
    "    opt0 = optimize.minimize(logp, [1, 1], method='Nelder-Mead', options={'maxiter':2000, 'disp':0}) # downhill simplex\n",
    "    return opt0\n",
    "opts = gethyperparams(xt, f(xt), np.ones_like(xt)*1e-12, K_SquareExp)\n",
    "for o in [opts]:\n",
    "    print(f'\\t chi2 = {o[\"fun\"]:12.6e}, niter = {o[\"nit\"]:5d}, Converged = {o[\"success\"]:6b} : {o[\"message\"]}')\n",
    "    print(f'\\t\\t η = {o[\"x\"][0]:6.3f}, l = {o[\"x\"][1]:6.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ηopt, lopt = opts['x']\n",
    "μopt, Kopt = GPpost(xt, f(xt), x, lambda x : 0.0, lambda x,y:ηopt**2*K_SquareExp(x,y,l=lopt), np.ones_like(xt)*1e-12)\n",
    "σopt       = np.sqrt(np.diag(Kopt))\n",
    "ytest      = np.array([GPfast(μopt, Kopt, np.ones_like(x)*1e-12) for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.plot(x, f(x))\n",
    "ax.plot(xt, f(xt), marker='o', mfc='None', ls='None', mew=2, color=\"C0\", label='$f$')\n",
    "ax.plot(xs, f(xs), marker='s', mfc='None', ls='None', mew=3, color=\"C3\", label='$f^\\prime$')\n",
    "for x0 in xs:\n",
    "    fprime = tangent(f,x0)\n",
    "    dx     = np.linspace(x0-0.3, x0+0.3)\n",
    "    ax.plot(dx,fprime(dx), color=\"C3\",lw=5,alpha=0.8)\n",
    "\n",
    "\n",
    "l, = ax.plot(x, μopt, alpha=0.6, label='GP(f)')\n",
    "ax.fill_between(x, μopt - 2*σopt, μopt + 2*σopt, color=l.get_color(), alpha=0.4) # avg+/-2sigma\n",
    "for y in ytest:\n",
    "    ax.plot(x, y, lw=0.2, color=l.get_color(), alpha=0.5)\n",
    "    \n",
    "ax.set_ylim(-4,2)\n",
    "ax.legend(fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what happens when we include information on the derivatives ($y^\\prime$).\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\n",
    "y \\\\\n",
    "y^\\prime \\\\\n",
    "y^\\star\n",
    "\\end{pmatrix}\\sim\\mathcal{N}\\left(0, \\begin{pmatrix}\n",
    "K_{yy} & K_{y y^\\prime} & K_{y y^\\star} \\\\\n",
    "K_{y^\\prime y} & K_{y^\\prime y^\\prime} & K_{y^\\prime y^\\star} \\\\\n",
    "K_{y^\\star y} & K_{y^\\star y^\\prime} &K_{y^\\star y^\\star}\n",
    "\\end{pmatrix}\\right)\n",
    "\\end{align}\n",
    "\n",
    "Recall that training datay are composed now of $y$ (defined on $x$) and $y^\\prime$ (defined on $x^\\prime$), and that the test data $y^\\star$ is defined on $x^\\star$.\n",
    "Let us group the training set into a super set, such that $\\widetilde{x} = (x, x^\\prime)$\n",
    "\n",
    "\\begin{align}\n",
    "y^\\star \\lvert y,y^\\prime \\sim \\mathcal{N}\\left( \\Sigma_\\star \\Sigma^{-1} \\begin{pmatrix}y\\\\y^\\prime\\end{pmatrix} ,\n",
    "\\Sigma_{\\star\\star} - \\Sigma_\\star \\Sigma^{-1} \\Sigma_\\star^t\\right)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\Sigma(\\widetilde{x}, \\widetilde{x}) &= \\begin{pmatrix}K_{yy}(x,x) & K_{yy^\\prime}(x,x^\\prime) \\\\ K_{y^\\prime y}(x^\\prime,x) & K_{y^\\prime y^\\prime}(x^\\prime,x^\\prime)\\end{pmatrix}\\\\\n",
    "\\Sigma_\\star(x^\\star, \\widetilde{x}) &= \\begin{pmatrix}K_{y^\\star y}(x^\\star, x) K_{y^\\star y^\\prime}(x^\\star, x^\\prime)\\end{pmatrix} \\\\\n",
    "\\Sigma_{\\star\\star}(x^\\star, x^\\star) &= K_{y^\\star y^\\star}(x^\\star, x^\\star)\n",
    "\\end{align}\n",
    "\n",
    "Given the linearity of the problem, and the GP properties, we express all covariance kernels in terms of $K_{yy}$\n",
    "\n",
    "\\begin{align}\n",
    "K_{y y^\\prime}(x,x^\\prime) &= \\left\\langle y(x) y^\\prime(x^\\prime)\\right\\rangle\n",
    "= \\left\\langle y(x) \\frac{\\partial}{\\partial x^\\prime} y(x^\\prime)\\right\\rangle \\\\\n",
    "&= \\frac{\\partial}{\\partial x^\\prime}\\left\\langle y(x) y(x^\\prime)\\right\\rangle \\equiv \\frac{\\partial}{\\partial x^\\prime} K_{yy}(x,x^\\prime) \\\\\n",
    "K_{y^\\prime y^\\prime}(x_1^\\prime, x_2^\\prime) &= \\left\\langle y^\\prime(x_1^\\prime) y^\\prime(x_2^\\prime)\\right\\rangle \\equiv \\frac{\\partial}{\\partial x_1^\\prime} \\frac{\\partial}{\\partial x_2^\\prime} K_{yy}(x_1^\\prime, x_2^\\prime) \\\\\n",
    "K_{y^\\star y^\\star}(x_1^\\star, x_2^\\star) &=\\left\\langle y^\\star(x_1^\\star) y^\\star(x_2^\\star)\\right\\rangle \\equiv K_{yy}(x_1^\\star, x_2^\\star)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "K_{y y^\\prime}(x, x^\\prime) &= \\frac{\\eta^2}{l^2}\\left(x - x^\\prime\\right) \\exp{\\left[-\\frac{(x-x^\\prime)^2}{2 l^2}\\right]} &=& \\frac{x - x^\\prime}{l^2} K_{yy}(x,x^\\prime)\\\\\n",
    "K_{y^\\prime, y^\\prime}(x_1^\\prime, x_2^\\prime) &= \\frac{\\eta^2}{l^2}\\left(1 - \\frac{(x_1^\\prime - x_2^\\prime)^2}{l^2}\\right) \\exp{\\left[-\\frac{\\left(x_1^\\prime - x_2^\\prime\\right)^2}{2l^2}\\right]} &=& \\frac{1}{l^2}\\left(1 - \\frac{\\left(x_1^\\prime - x_2^\\prime\\right)^2}{l^2}\\right) K_{yy}(x_1^\\prime, x_2^\\prime)\n",
    "\\end{align}\n",
    "\n",
    "We can learn the hyperparameters $(\\eta, l)$ by maximizing the posterior, but now\n",
    "\n",
    "\\begin{align}\n",
    "\\log{P(\\eta,l | D, I)} &= -\\frac{1}{2}\\left[\\begin{pmatrix}y &y^\\prime\\end{pmatrix} K_{yy^\\prime}^{-1}(\\eta, l) \\begin{pmatrix}y\\\\ y^\\prime\\end{pmatrix} + \\log{\\det{K_{yy^\\prime}(\\eta,l)}} \\right] \n",
    "- \\left[\\log{l} + \\log{\\eta}\\right] + \\text{const}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getK_obs_obs(Koo, xo_f, xo_df, l = 1.0):\n",
    "    # ....\n",
    "    return Koo\n",
    "def getK_new_obs(Kno, xo_f, xo_df, xn, l = 1.0):\n",
    "    # ....\n",
    "    return Kno\n",
    "def getK_new_new(Knn, xn, l=1.0):\n",
    "    # ...\n",
    "    return Knn\n",
    "\n",
    "def gethyperparams2(x_f, x_df, f, df, noise):\n",
    "    # ...\n",
    "    def logGP(*,η,l):\n",
    "        # ...\n",
    "    # ...\n",
    "    return opt0\n",
    "\n",
    "opts2 = # ...\n",
    "for o in [opts2]:\n",
    "    print(f'\\t chi2 = {o[\"fun\"]:12.6e}, niter = {o[\"nit\"]:5d}, Converged = {o[\"success\"]:6b} : {o[\"message\"]}')\n",
    "    print(f'\\t\\t η = {o[\"x\"][0]:6.3f}, l = {o[\"x\"][1]:6.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ηopt2, lopt2 = opts2['x']\n",
    "K_obs_obs    = np.zeros((nums['obs'], nums['obs']))\n",
    "K_new_obs    = np.zeros((nums['new'], nums['obs']))\n",
    "K_new_new    = np.zeros((nums['new'], nums['new']))\n",
    "\n",
    "K_obs_obs[...] = # ...\n",
    "K_new_obs[...] = # ...\n",
    "K_new_new[...] = # ...\n",
    "\n",
    "μopt2, Kopt2 = # ...\n",
    "σopt2        = # ...\n",
    "ytest2       = # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "l, = ax.plot(x, μopt, alpha=0.6, label=r'$GP(f)$', color='C1')\n",
    "ax.fill_between(x, μopt - 2*σopt, μopt + 2*σopt, color=l.get_color(), alpha=0.4) # avg+/-2sigma\n",
    "for y in ytest:\n",
    "    ax.plot(x, y, lw=0.2, color=l.get_color(), alpha=0.8)\n",
    "    \n",
    "l, = ax.plot(x, μopt2, alpha=0.6, label=r'$GP(f,f^\\prime)$', color='C4')\n",
    "ax.fill_between(x, μopt2 - 2*σopt2, μopt2 + 2*σopt2, color=l.get_color(), alpha=0.4) # avg+/-2sigma\n",
    "for y in ytest2:\n",
    "    ax.plot(x, y, lw=0.2, color=l.get_color(), alpha=0.8)\n",
    "    \n",
    "ax.plot(x, f(x), color='C0')\n",
    "ax.plot(xt, f(xt), marker='o', mfc='None', ls='None', mew=2, color=\"C0\", label=r'training $f$')\n",
    "ax.plot(xs, f(xs), marker='s', mfc='None', ls='None', mew=3, color=\"C3\", label=r'training $f^\\prime$')\n",
    "for x0 in xs:\n",
    "    fprime = tangent(f,x0)\n",
    "    dx     = np.linspace(x0-0.3, x0+0.3)\n",
    "    ax.plot(dx,fprime(dx), color=\"C3\",lw=5,alpha=0.8)\n",
    "    \n",
    "ax.set_ylim(-12,12)\n",
    "ax.legend(fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
